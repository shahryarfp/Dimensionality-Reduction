# -*- coding: utf-8 -*-
"""q3_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1e9UdxG5s1-L3ERGlP4xhcTPQgi7ZFL2T
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import random
from skimage import color
# from sklearn.metrics import confusion_matrix
from sklearn import metrics
import seaborn as sns

"""### Loading datas"""

# loading datas
from keras.datasets import cifar10
(x_train,y_train) , (x_test,y_test) = cifar10.load_data()
reshaped_all_datas = np.concatenate((x_train,x_test))

"""### Plotting 10 random images"""

#plotting 10 random images from batch1
randomlist = random.sample(range(0, len(reshaped_all_datas)), 10)

for i in range(10):
    plt.subplot(4,4,i+1)
    plt.imshow(reshaped_all_datas[randomlist[i]])
plt.show()

"""We have three types of datas: <br>
1- training <br>
2- validation <br>
3- test <br> <br>

Also we have three types of training datas <br>
1- Batch based : All data pints are applied, simultaneously <br>
2- Stochastic based: Data are shuffled and then data points are applied one by one <br>
3- Stochastic mini batch based: training data set are randomly divided to a certain number of subsets (minbatches); then at each time a min-batch is chosen and applied <br><br>

### Pre Processing
"""

#converting RGB to Grayscale (takes too much time)
def rgb2gray(rgb):
    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]
    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b
    return gray

temp = []
for i in range(len(reshaped_all_datas)):
    img = reshaped_all_datas[i]
    ig = rgb2gray(img)
    ig = ig.reshape(1,32,32)
    temp.extend(ig.tolist())
reshaped_all_datas = np.array(temp)

#finding max and min in all datas
all_data_max = np.amax(reshaped_all_datas)
all_data_min = np.amin(reshaped_all_datas)
#normalizing datas
normalized_all_datas = reshaped_all_datas.astype('float32')/(all_data_max - all_data_min)

"""### MLP

##### Creating Dataset
"""

Y = np.concatenate((y_train,y_test)) #target
X = normalized_all_datas #input
(x_train,x_valid,x_test) = X[:40000],X[40000:50000],X[50000:]
(y_train,y_valid,y_test) = Y[:40000],Y[40000:50000],Y[50000:]

#uncomment if rgb to grayscale used
x_train = x_train.reshape(x_train.shape[0],32,32,1)
x_valid = x_valid.reshape(x_valid.shape[0],32,32,1)
x_test = x_test.reshape(x_test.shape[0],32,32,1)

y_train = tf.keras.utils.to_categorical(y_train,10)
y_valid = tf.keras.utils.to_categorical(y_valid,10)
y_test = tf.keras.utils.to_categorical(y_test,10)

"""#### Creating Model"""

from keras import layers
model = tf.keras.Sequential()

activation_func = 'relu'
# activation_func = tf.keras.activations.tanh
# activation_func = tf.keras.activations.sigmoid
# activation_func = tf.keras.activations.softmax

#CNN
# model.add(layers.Conv2D(filters=35, kernel_size=2, padding='same', activation=activation_func, input_shape=(32,32,1))) 
# model.add(layers.MaxPooling2D(pool_size=2))
# model.add(layers.Dropout(0.2))
# model.add(layers.Conv2D(filters=50, kernel_size=2, padding='same', activation=activation_func))
# model.add(layers.MaxPooling2D(pool_size=2))
# model.add(layers.Dropout(0.2))
# model.add(layers.Flatten())


model.add(layers.Flatten(input_shape = (32, 32, 1)))
model.add(layers.Dense(500, activation=activation_func))
model.add(layers.Dropout(0.3))

model.add(layers.Dense(400, activation=activation_func))
model.add(layers.Dropout(0.3))

# model.add(layers.Dense(150, activation=activation_func))
# model.add(layers.Dropout(0.3))

# model.add(layers.Dense(150, activation=activation_func))
# model.add(layers.Dropout(0.4))

# model.add(layers.Dense(150, activation=activation_func))
# model.add(layers.Dropout(0.3))

#last layer
model.add(layers.Dense(10, activation='softmax'))

"""#### Compiling the Model"""

loss_func = 'categorical_crossentropy'
# loss_func = 'mean_squared_error'
# loss_func = 'mean_absolute_error'

optimizer_func = 'adam'
# optimizer_func = tf.keras.optimizers.SGD(learning_rate = 0.1)
# optimizer_func = tf.keras.optimizers.Adagrad(learning_rate = 0.1)

model.compile(loss=loss_func,
              optimizer=optimizer_func,
              metrics=['accuracy'])

"""#### Training Model"""

import datetime
start = datetime.datetime.now()
trainedModel = model.fit(x_train, y_train, batch_size=512, epochs=50, validation_data = (x_valid,y_valid))
end = datetime.datetime.now()

print('Training Duration: ', end-start)

"""#### Evaluating the Model"""

history = trainedModel.history
loss = history['loss']
val_loss = history['val_loss']
acc = history['accuracy']
val_acc = history['val_accuracy']

plt.xlabel('Epochs')
plt.ylabel('loss')
plt.plot(loss)
plt.plot(val_loss)
plt.legend(['loss','val_loss'])
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('accuracy')
plt.plot(acc)
plt.plot(val_acc)
plt.legend(['acc','val_acc'])

test_loss,test_accuracy = model.evaluate(x_test, y_test)
print(test_loss)
print(test_accuracy)

#confusion matrix
y_pred = model.predict(x_test)
y_pred_temp = []

for i in range(len(y_pred)):
  pred_index = np.argmax(y_pred[i])
  y_pred_temp.append(pred_index)
y_test_temp = []
for i in range(len(y_test)):
  test_index = np.argmax(y_test[i])
  y_test_temp.append(test_index)

matrix_confusion = metrics.confusion_matrix(y_test_temp, y_pred_temp)
mc = sns.heatmap(matrix_confusion, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)
mc.set_title('Confusion Matrix\n');
mc.set_xlabel('\nPredicted Values')
mc.set_ylabel('Actual Values ');
plt.show()

#calculating other metrics
print(metrics.classification_report(y_test_temp, y_pred_temp, digits=3))

"""  ## PCA"""

#Reshaping datas
x_train_pca = x_train.reshape(x_train.shape[0],32*32)
# y_train_pca = y_train.reshape(y_train.shape[0],32*32)

x_valid_pca = x_valid.reshape(x_valid.shape[0],32*32)
# y_valid_pca = y_valid.reshape(y_valid.shape[0],32*32)

x_test_pca = x_test.reshape(x_test.shape[0],32*32)
# y_test_pca = y_test.reshape(y_test.shape[0],32*32)

# print(x_train_pca.shape)

from numpy import linalg as la 
covmat_train = np.cov(x_train_pca.T)
value_train , vector_train = la.eig(covmat_train)
sortedavlue_train = np.sort(value_train)[::-1]
ind_train=[]
for i in range(len(sortedavlue_train)):
  for j in range(len(value_train)):
    if sortedavlue_train[i] == value_train[j]:
      ind_train.append(j)
      break
ind_train = ind_train[:400]
vector_train = vector_train[ind_train]
encoded_train = np.matmul(x_train_pca, vector_train.T)
encoded_test = np.matmul(x_test_pca, vector_train.T)
encoded_valid = np.matmul(x_valid_pca, vector_train.T)

"""#### Creating Model"""

from keras import layers
model = tf.keras.Sequential()

activation_func = 'relu'
# activation_func = tf.keras.activations.tanh
# activation_func = tf.keras.activations.sigmoid
# activation_func = tf.keras.activations.softmax

model.add(layers.Flatten(input_shape = (400, 1)))
model.add(layers.Dense(400, activation=activation_func))
model.add(layers.Dropout(0.3))

# #using more layer between encoder and decoder
# model.add(layers.Dense(600, activation=activation_func))
# model.add(layers.Dropout(0.3))

model.add(layers.Dense(700, activation=activation_func))
model.add(layers.Dropout(0.3))

#last layer
model.add(layers.Dense(10, activation='softmax'))

"""#### Compiling the Model"""

loss_func = 'categorical_crossentropy'
# loss_func = 'mean_squared_error'
# loss_func = 'mean_absolute_error'

optimizer_func = 'adam'
# optimizer_func = tf.keras.optimizers.SGD(learning_rate = 0.1)
# optimizer_func = tf.keras.optimizers.Adagrad(learning_rate = 0.1)

model.compile(loss=loss_func,
              optimizer=optimizer_func,
              metrics=['accuracy'])

"""#### Training the Model"""

import datetime
start = datetime.datetime.now()
trainedModel = model.fit(encoded_train, y_train, batch_size=512, epochs=50, validation_data = (encoded_valid,y_valid))
end = datetime.datetime.now()

history = trainedModel.history
loss = history['loss']
val_loss = history['val_loss']
acc = history['accuracy']
val_acc = history['val_accuracy']

plt.xlabel('Epochs')
plt.ylabel('loss')
plt.plot(loss)
plt.plot(val_loss)
plt.legend(['loss','val_loss'])
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('accuracy')
plt.plot(acc)
plt.plot(val_acc)
plt.legend(['acc','val_acc'])

test_loss,test_accuracy = model.evaluate(encoded_test, y_test)
print(test_loss)
print(test_accuracy)

#confusion matrix
y_pred = model.predict(encoded_test)
y_pred_temp = []

for i in range(len(y_pred)):
  pred_index = np.argmax(y_pred[i])
  y_pred_temp.append(pred_index)
y_test_temp = []
for i in range(len(y_test)):
  test_index = np.argmax(y_test[i])
  y_test_temp.append(test_index)

matrix_confusion = metrics.confusion_matrix(y_test_temp, y_pred_temp)
mc = sns.heatmap(matrix_confusion, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)
mc.set_title('Confusion Matrix\n');
mc.set_xlabel('\nPredicted Values')
mc.set_ylabel('Actual Values ');
plt.show()

#calculating other metrics
print(metrics.classification_report(y_test_temp, y_pred_temp, digits=3))

"""## Auto-Encoder

#### Creating Model
"""

from keras import layers
model = tf.keras.Sequential()

activation_func = 'relu'
# activation_func = tf.keras.activations.tanh
# activation_func = tf.keras.activations.sigmoid
# activation_func = tf.keras.activations.softmax

model.add(layers.Flatten(input_shape = (32, 32, 1)))
model.add(layers.Dense(400, activation=activation_func))
model.add(layers.Dropout(0.3))

#using more layer between encoder and decoder
model.add(layers.Dense(600, activation=activation_func))
model.add(layers.Dropout(0.3))

model.add(layers.Dense(1024, activation=activation_func))
model.add(layers.Dropout(0.3))

#last layer
model.add(layers.Dense(10, activation='softmax'))

"""#### Compiling the Model"""

loss_func = 'categorical_crossentropy'
# loss_func = 'mean_squared_error'
# loss_func = 'mean_absolute_error'

optimizer_func = 'adam'
# optimizer_func = tf.keras.optimizers.SGD(learning_rate = 0.1)
# optimizer_func = tf.keras.optimizers.Adagrad(learning_rate = 0.1)

model.compile(loss=loss_func,
              optimizer=optimizer_func,
              metrics=['accuracy'])

"""#### Training the Model"""

import datetime
start = datetime.datetime.now()
trainedModel = model.fit(x_train, y_train, batch_size=512, epochs=50, validation_data = (x_valid,y_valid))
end = datetime.datetime.now()

history = trainedModel.history
loss = history['loss']
val_loss = history['val_loss']
acc = history['accuracy']
val_acc = history['val_accuracy']

plt.xlabel('Epochs')
plt.ylabel('loss')
plt.plot(loss)
plt.plot(val_loss)
plt.legend(['loss','val_loss'])
plt.figure()
plt.xlabel('Epochs')
plt.ylabel('accuracy')
plt.plot(acc)
plt.plot(val_acc)
plt.legend(['acc','val_acc'])

test_loss,test_accuracy = model.evaluate(x_test, y_test)
print(test_loss)
print(test_accuracy)

#confusion matrix
y_pred = model.predict(x_test)
y_pred_temp = []

for i in range(len(y_pred)):
  pred_index = np.argmax(y_pred[i])
  y_pred_temp.append(pred_index)
y_test_temp = []
for i in range(len(y_test)):
  test_index = np.argmax(y_test[i])
  y_test_temp.append(test_index)

matrix_confusion = metrics.confusion_matrix(y_test_temp, y_pred_temp)
mc = sns.heatmap(matrix_confusion, square=True, annot=True, cmap='Blues', fmt='d', cbar=False)
mc.set_title('Confusion Matrix\n');
mc.set_xlabel('\nPredicted Values')
mc.set_ylabel('Actual Values ');
plt.show()

#calculating other metrics
print(metrics.classification_report(y_test_temp, y_pred_temp, digits=3))

"""## Correlation Matrix for question 2 data set"""

import pandas as pd

my_file = pd.read_csv('data.csv')
sns.heatmap(my_file.corr())

"""## Linear Regression Compairing"""

my_file.date = my_file.date.astype("category").cat.codes
my_file.street = my_file.street.astype("category").cat.codes
my_file.city = my_file.city.astype("category").cat.codes
my_file.statezip = my_file.statezip.astype("category").cat.codes
my_file.country = my_file.country.astype("category").cat.codes

my_file = (my_file-my_file.min() + 1)/(my_file.max()-my_file.min() + 1)
y = my_file['price']
x = my_file.drop(['price'],axis=1)

from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(x, y)

features = pd.DataFrame(regressor.coef_, x.columns, columns=['coefficient'])
features.coefficient = features.coefficient.abs()

stdevs = []
for i in x.columns:
    stdev = my_file[i].std()
    stdevs.append(stdev)
features["stdev"] = np.array(stdevs).reshape(-1,1)
features["importance"] = features["coefficient"] * features["stdev"]
features['importance_normalized'] = 100*features['importance'] / features['importance'].max()
print(features['importance_normalized'])
plt.barh(features.index, features.importance_normalized)

"""### Decision Tree Compairing"""

from sklearn.tree import DecisionTreeRegressor
model = DecisionTreeRegressor()
model.fit(x, y)
importance = model.feature_importances_
importance = 100*importance / importance.max()
plt.barh(x.columns, importance)



